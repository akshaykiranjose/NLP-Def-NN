# -*- coding: utf-8 -*-
"""project2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jCK_92CYc69N1Qakv6iSnTEGCuMxZzUN
"""

import numpy as np
from scipy.optimize import linprog
from scipy.sparse.linalg import spsolve
import cvxpy as cp
import imageio
import pandas as pd
import math
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from tensorflow.keras.optimizers import SGD
from matplotlib import pyplot

from google.colab import drive
drive.mount('/content/drive')

data_path = "/content/drive/MyDrive/Colab Notebooks/mfds project 2/data_set.data"

headers = ["symboling","normalized-losses","make","fuel-type","aspiration", "num-of-doors","body-style",
          "drive-wheels","engine-location","wheel-base", "length","width","height","curb-weight","engine-type",
          "num-of-cylinders", "engine-size","fuel-system","bore","stroke","compression-ratio","horsepower",
          "peak-rpm","city-mpg","highway-mpg","price"]

df = pd.read_csv(data_path, names = headers)
df.shape

df.info

pd.set_option('display.max_rows', None,'display.max_columns',None)
df.head(204)

pd.set_option('display.max_rows',20)
df.replace('?',np.nan,inplace=True)
miss_data=df.isnull()
display(miss_data.sum())
miss_data_col=["normalized-losses","bore","stroke","horsepower","peak-rpm","price"]
for c in miss_data_col:
    avg=df[c].astype("float").mean(axis=0)
    df[c].replace(np.nan,avg,inplace=True)
pd.set_option('display.max_rows', 10,'display.max_columns', None)
display(df)

df.info()

df["num-of-doors"].replace(np.nan,df["num-of-doors"].value_counts().idxmax(),inplace =True ) 
print(df.isnull().sum())
df[["bore"]] = df[["bore"]].astype("float")
df[["stroke"]] = df[["stroke"]].astype("float")
df[["normalized-losses"]] = df[["normalized-losses"]].astype("int")
df[["price"]] = df[["price"]].astype("float")
df[["peak-rpm"]] = df[["peak-rpm"]].astype("float")
df[["horsepower"]] = df[["horsepower"]].astype("float")
df.info()

df["num-of-doors"] = df["num-of-doors"].apply(lambda x: 4 if x == 'four' else 2)
df.replace({'four': 4,'six': 6, 'five': 5, 'three': 3, 'twelve': 12, 'two': 2, 'eight': 8},inplace=True)

for i in ['make','fuel-type','aspiration','body-style','drive-wheels','engine-location','engine-type','fuel-system']:
  codes=None
  unique=None
  codes, uniques = pd.factorize(df[i])
  df[i]=codes

display(df)

df.info()

df=df.astype("float")
y = df['symboling'].copy()
X = df.drop('symboling', axis=1).copy()
scaler = StandardScaler()
X = scaler.fit_transform(X)
trainX, testX, trainy, testy = train_test_split(X, y, train_size=0.7, random_state=100)

# define model
model = Sequential()
model.add(Dense(100, input_dim=25, activation='relu', kernel_initializer='he_uniform'))
model.add(Dense(1, activation='softmax'))
# compile model
opt = SGD(lr=0.01, momentum=0.9)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
# fit model
history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)
# evaluate the model
_, train_acc = model.evaluate(trainX, trainy, verbose=0)
_, test_acc = model.evaluate(testX, testy, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))
# plot loss during training
pyplot.subplot(211)
pyplot.title('Loss')
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
# plot accuracy during training
pyplot.subplot(212)
pyplot.title('Accuracy')
pyplot.plot(history.history['accuracy'], label='train')
pyplot.plot(history.history['val_accuracy'], label='test')
pyplot.legend()
pyplot.show()

df.info()

inputs = tf.keras.Input(shape=(X.shape[1],))
x = tf.keras.layers.Dense(128, activation='relu')(inputs)
x = tf.keras.layers.Dense(128, activation='relu')(x)
outputs = tf.keras.layers.Dense(1, activation='softmax')(x)

model = tf.keras.Model(inputs, outputs)

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

batch_size = 32
epochs = 20

history = model.fit(
    trainX,
    trainy,
    validation_split=0.2,
    batch_size=batch_size,
    epochs=epochs
)